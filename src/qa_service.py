"""
Q&A ì„œë¹„ìŠ¤ - RAG (Retrieval-Augmented Generation)
ê²€ìƒ‰ëœ ê´€ë ¨ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ LLMì´ ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì„œë¹„ìŠ¤
"""

import requests
import json
from typing import List, Dict, Any, Optional
from loguru import logger
from src.config import config

from src.search_service import SearchService
from src.embedding_service import EmbeddingService
from src.qdrant_manager import QdrantManager


class QAService:
    def __init__(self):
        """Q&A ì„œë¹„ìŠ¤ ì´ˆê¸°í™”"""
        self.ollama_host = config.OLLAMA_HOST
        self.llm_model = "gemma3:latest"  # Gemma3 ëª¨ë¸ ì‚¬ìš©
        self.search_service = SearchService()
        self.embedding_service = EmbeddingService()
        
        logger.info(f"Q&A ì„œë¹„ìŠ¤ ì´ˆê¸°í™”: {self.llm_model}")
    
    def generate_answer(self, query: str, context: List[str], max_tokens: int = 500, history: list = None) -> str:
        """LLMì„ ì‚¬ìš©í•˜ì—¬ ë‹µë³€ ìƒì„± (ì´ì „ ëŒ€í™” history ë°˜ì˜)"""
        try:
            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = self._build_prompt(query, context, history)
            # Ollama API í˜¸ì¶œ
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "max_tokens": max_tokens
                    }
                },
                timeout=120
            )
            if response.status_code == 200:
                result = response.json()
                answer = result.get("response", "").strip()
                logger.info(f"ë‹µë³€ ìƒì„± ì™„ë£Œ: {len(answer)}ì")
                return answer
            else:
                logger.error(f"LLM API ì˜¤ë¥˜: {response.status_code}")
                return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
        except Exception as e:
            logger.error(f"ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ì„ ìƒì„±í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
    
    def _build_prompt(self, query: str, context: List[str], history: list = None) -> str:
        """RAG í”„ë¡¬í”„íŠ¸ êµ¬ì„± - ìƒí™©ë³„ ìµœì í™” ë²„ì „ (í‘œ/ë¦¬ìŠ¤íŠ¸/ê·¼ê±°/ì¶œì²˜ ë“±)"""
        import re
        filtered_context = []
        context_types = []
        # í‘œ/ë¦¬ìŠ¤íŠ¸/ì¼ë°˜ í…ìŠ¤íŠ¸ êµ¬ë¶„ ë° í•„í„°ë§
        for block in context:
            lines = [l.strip() for l in block.splitlines() if l.strip()]
            if not lines:
                context_types.append("text")
                filtered_context.append(block)
                continue
            if lines[0].startswith("|") and lines[0].endswith("|"):
                context_types.append("table")
                filtered_context.append(block)
            elif all(l.startswith("-") or l[0].isdigit() for l in lines):
                context_types.append("list")
                filtered_context.append(block)
            else:
                context_types.append("text")
                filtered_context.append(block)
        # ì˜ˆì‹œ: 'ì§„í•™ìˆ˜ 40ëª… ì´ìƒ' íŒ¨í„´ ê°ì§€ ë° í‘œ í•„í„°ë§
        m = re.search(r"([ê°€-í£A-Za-z0-9_]+)\s*([0-9]+)ëª…\s*ì´ìƒ", query)
        if m:
            col_name = m.group(1)
            min_val = int(m.group(2))
            new_context = []
            for block, ctype in zip(filtered_context, context_types):
                if ctype != "table":
                    new_context.append(block)
                    continue
                lines = [l.strip() for l in block.splitlines() if l.strip()]
                header = [h.strip() for h in lines[0].strip("|").split("|")]
                try:
                    col_idx = header.index(col_name)
                except ValueError:
                    new_context.append(block)
                    continue
                filtered_rows = [lines[0]]
                for row in lines[1:]:
                    cells = [c.strip() for c in row.strip("|").split("|")]
                    try:
                        if int(cells[col_idx]) >= min_val:
                            filtered_rows.append(row)
                    except:
                        continue
                if len(filtered_rows) > 1:
                    new_context.append("\n".join(filtered_rows))
            filtered_context = new_context
        # ìƒí™©ë³„ í”„ë¡¬í”„íŠ¸ ë¶„ê¸° (ìì—°ìŠ¤ëŸ¬ìš´ ìƒë‹´ì› ìŠ¤íƒ€ì¼ ê°•ì¡°)
        context_text = "\n\n".join(filtered_context)
        style_guide = (
            "- ë‹µë³€ì€ í‘œ/ë¦¬ìŠ¤íŠ¸/ë°ì´í„° ë³µì‚¬ ëŠë‚Œì´ ì•„ë‹ˆë¼, ì‹¤ì œ ìƒë‹´ì›ì´ ì•ˆë‚´í•˜ëŠ” ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ì‘ì„±í•´ ì£¼ì„¸ìš”.\n"
            "- í•„ìš”ì‹œ ì˜ˆì‹œ, ì¶”ê°€ ì„¤ëª…ë„ í¬í•¨í•´ ì£¼ì„¸ìš”.\n"
            "- í‘œê°€ í¬í•¨ëœ ê²½ìš° ë§ˆí¬ë‹¤ìš´ í‘œ í˜•ì‹ìœ¼ë¡œ, ë¦¬ìŠ¤íŠ¸ëŠ” ë²ˆí˜¸ ë˜ëŠ” ê¸°í˜¸ë¡œ, ì¼ë°˜ í…ìŠ¤íŠ¸ëŠ” ìì—°ìŠ¤ëŸ½ê²Œ ìš”ì•½í•´ ì£¼ì„¸ìš”.\n"
            "- ë‹µë³€ì—ëŠ” ë°˜ë“œì‹œ ê´€ë ¨ ê·¼ê±°(ì¶œì²˜, ë¬¸ì„œëª…, ì‹œíŠ¸ëª…, í–‰/ì—´, í˜ì´ì§€ ë“±)ë¥¼ ëª…í™•íˆ í‘œê¸°í•˜ì„¸ìš”."
        )
        # history(ì´ì „ ëŒ€í™”)ê°€ ìˆìœ¼ë©´ í”„ë¡¬í”„íŠ¸ ìƒë‹¨ì— ì¶”ê°€
        history_text = ""
        if history and isinstance(history, list) and len(history) > 0:
            history_lines = []
            for turn in history:
                q = turn.get("question") or turn.get("content") or turn.get("user") or ""
                a = turn.get("answer") or turn.get("response") or turn.get("assistant") or ""
                if q:
                    history_lines.append(f"ì´ì „ ì§ˆë¬¸: {q}")
                if a:
                    history_lines.append(f"ì´ì „ ë‹µë³€: {a}")
            if history_lines:
                history_text = "\n".join(history_lines) + "\n"
        prompt = f"""{history_text}ì•„ë˜ ì°¸ê³  ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ëŒ€í•´ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ ì£¼ì„¸ìš”.\n{style_guide}\n\n[ì°¸ê³  ë‚´ìš©]\n{context_text}\n\n[ì§ˆë¬¸]\n{query}\n[ë‹µë³€]"""
        return prompt
    
    def ask_question(self, question: str, collection_name: str = "pdf_documents", 
                    max_results: int = 5, max_tokens: int = 500, document_id: str = None, history=None) -> Dict[str, Any]:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ì¶œì²˜/ê·¼ê±° ì •ë³´ í¬í•¨)"""
        try:
            logger.info(f"ì§ˆë¬¸ ì²˜ë¦¬ ì‹œì‘: {question}")
            # 1. ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰
            search_service = self.search_service
            if collection_name:
                search_service = SearchService(QdrantManager(collection_name=collection_name), self.embedding_service)
            search_results = search_service.search(
                query=question,
                limit=max_results,
                document_id=document_id
            )
            logger.info(f"ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ")
            if not search_results:
                logger.warning(f"ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ: {question}")
                return {
                    "question": question,
                    "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ê´€ë ¨ëœ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                    "sources": [],
                    "search_results": []
                }
            # 2. ê²€ìƒ‰ëœ í…ìŠ¤íŠ¸ ì¶”ì¶œ ë° ì¶œì²˜ ë©”íƒ€ë°ì´í„° ìˆ˜ì§‘
            context_texts = []
            sources = []
            for result in search_results:
                text = result.get("text", "")
                if text:
                    context_texts.append(text)
                    meta_dict = {}
                    for k in ["document_id", "title", "sheet", "row", "page_number", "chunk_index"]:
                        if "metadata" in result and k in result["metadata"]:
                            meta_dict[k] = result["metadata"][k]
                        elif k in result:
                            meta_dict[k] = result[k]
                    if meta_dict:
                        sources.append(meta_dict)
            # 3. LLMì„ ì‚¬ìš©í•œ ë‹µë³€ ìƒì„±
            answer = self.generate_answer(question, context_texts, max_tokens, history)
            # 4. ë‹µë³€ì— ì¶œì²˜ ì¶”ê°€ (ìì—°ì–´ ê·¼ê±°)
            if sources:
                readable_sources = []
                for i, src in enumerate(sources):
                    # í•´ë‹¹ ì²­í¬ì˜ í…ìŠ¤íŠ¸ ì¼ë¶€ ì¶”ì¶œ
                    chunk_text = search_results[i].get("text", "")
                    chunk_preview = chunk_text.strip().replace("\n", " ")[:80] + ("..." if len(chunk_text) > 80 else "")
                    doc_name = src.get("title") or src.get("document_id", "ë¬¸ì„œ")
                    page = src.get("page_number")
                    sheet = src.get("sheet")
                    # ìì—°ì–´ ê·¼ê±° ë¬¸ì¥ ìƒì„±
                    meta_parts = []
                    if doc_name:
                        meta_parts.append(f"ë¬¸ì„œ: {doc_name}")
                    if sheet:
                        meta_parts.append(f"ì‹œíŠ¸: {sheet}")
                    if page is not None:
                        meta_parts.append(f"í˜ì´ì§€: {page}")
                    readable = f"- \"{chunk_preview}\" ({', '.join(meta_parts)})"
                    readable_sources.append(readable)
                answer += f"\n\nğŸ“„ ê´€ë ¨ ì¶œì²˜:\n" + "\n".join(readable_sources)
            return {
                "question": question,
                "answer": answer,
                "sources": sources,
                "search_results": search_results,
                "context_count": len(context_texts)
            }
        except Exception as e:
            logger.error(f"ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "question": question,
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "sources": [],
                "search_results": [],
                "error": str(e)
            }
    
    def ask_with_metadata(self, question: str, collection_name: str = "pdf_documents", max_results: int = 5, max_tokens: int = 500, document_id: str = None) -> Dict[str, Any]:
        """ë©”íƒ€ë°ì´í„°ë¥¼ í¬í•¨í•œ ìƒì„¸í•œ ì§ˆë¬¸ ì²˜ë¦¬"""
        try:
            # ê¸°ë³¸ ì§ˆë¬¸ ì²˜ë¦¬
            result = self.ask_question(question, collection_name, max_results=max_results, max_tokens=max_tokens, document_id=document_id)
            
            # ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë¶„ì„
            if result.get("search_results"):
                # ë¬¸ì„œ ì •ë³´ ìˆ˜ì§‘
                documents = {}
                for search_result in result["search_results"]:
                    doc_id = search_result.get("document_id")
                    if doc_id not in documents:
                        documents[doc_id] = {
                            "document_id": doc_id,
                            "metadata": search_result.get("metadata", {}),
                            "chunks_count": 0
                        }
                    documents[doc_id]["chunks_count"] += 1
                
                result["documents"] = list(documents.values())
                
                # í†µê³„ ì •ë³´
                result["stats"] = {
                    "total_results": len(result["search_results"]),
                    "unique_documents": len(documents),
                    "avg_score": sum(r.get("score", 0) for r in result["search_results"]) / len(result["search_results"]) if result["search_results"] else 0
                }
            
            return result
            
        except Exception as e:
            logger.error(f"ë©”íƒ€ë°ì´í„° í¬í•¨ ì§ˆë¬¸ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
            return {
                "question": question,
                "answer": "ì£„ì†¡í•©ë‹ˆë‹¤. ì§ˆë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                "error": str(e)
            }
    
    def test_llm_connection(self) -> bool:
        """LLM ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            response = requests.post(
                f"{self.ollama_host}/api/generate",
                json={
                    "model": self.llm_model,
                    "prompt": "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ì…ë‹ˆë‹¤.",
                    "stream": False,
                    "options": {
                        "max_tokens": 10
                    }
                },
                timeout=60
            )
            
            if response.status_code == 200:
                logger.info("LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì„±ê³µ")
                return True
            else:
                logger.error(f"LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {response.status_code}")
                return False
                
        except Exception as e:
            logger.error(f"LLM ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def get_available_models(self) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            response = requests.get(f"{self.ollama_host}/api/tags")
            if response.status_code == 200:
                models = response.json().get("models", [])
                return [model["name"] for model in models]
            else:
                return []
        except Exception as e:
            logger.error(f"ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì¤‘ ì˜¤ë¥˜: {e}")
            return []


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_qa_service():
    """Q&A ì„œë¹„ìŠ¤ í…ŒìŠ¤íŠ¸"""
    qa_service = QAService()
    
    # LLM ì—°ê²° í…ŒìŠ¤íŠ¸
    if not qa_service.test_llm_connection():
        print("âŒ LLM ì—°ê²° ì‹¤íŒ¨")
        return
    
    print("âœ… LLM ì—°ê²° ì„±ê³µ")
    
    # í…ŒìŠ¤íŠ¸ ì§ˆë¬¸ë“¤
    test_questions = [
        "ìš´ìˆ˜ì¢‹ì€ë‚ ì— ë‚˜ì˜¤ëŠ” ë“±ì¥ì¸ë¬¼ ì´ë¦„ë“¤ì€?",
        "ê¹€ì²¨ì§€ëŠ” ì–´ë–¤ ì§ì—…ì„ ê°€ì§€ê³  ìˆë‚˜ìš”?",
        "ì¹˜ì‚¼ì€ ì–´ë–¤ ì¸ë¬¼ì¸ê°€ìš”?",
        "ê°œë˜¥ì´ëŠ” ëˆ„êµ¬ì¸ê°€ìš”?",
        "ì´ ì†Œì„¤ì˜ ë°°ê²½ì€ ì–´ë””ì¸ê°€ìš”?"
    ]
    
    for question in test_questions:
        print(f"\nğŸ” ì§ˆë¬¸: {question}")
        result = qa_service.ask_question(question)
        print(f"ğŸ’¡ ë‹µë³€: {result['answer']}")
        print(f"ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìˆ˜: {len(result['search_results'])}")


if __name__ == "__main__":
    test_qa_service()
